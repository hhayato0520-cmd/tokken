# tokken

# 1
# cl-tohoku/bert-base-japanese-v3 まとめ表

| 項目 | 内容 |
|------|-------|
| **モデル名** | cl-tohoku/bert-base-japanese-v3 |
| **入力形式** | MeCab（UniDic 2.1.2）で単語分割 → WordPiece サブワード化 |
| **用途（タスク）** | テキスト分類、ニュース見出し分類、感情分析、固有表現抽出、特徴抽出など |
| **モデル規模** | 約 111M パラメータ（BERT-base: 12層・768次元・12ヘッド） |
| **事前学習データ（1）** | CC-100（日本語）：約 74.3 GB（約 392M 文） |
| **事前学習データ（2）** | Wikipedia 日本語：約 4.9 GB（約 34M 文） |
| **訓練過程** | Masked Language Modeling（MLM）を採用し、日本語文の理解能力を向上させるために、文中の単語をマスクして予測するタスクで学習 |

# 2
# どのように使えそうか

### 🔍 **短文（例：ニュース見出し）に対する推論タスクの精度向上**
BERT-base-japanese-v3 は日本語文脈理解に強いが、複数ステップの推論はそこまで得意ではない。  
そこで、入力時に **人間が思考手順（CoTプロンプト）を与える**ことで、以下のような推論的タスクに適用できると考えられる。

- ニュース見出し → 「分野分類」（政治・経済・国際・スポーツ等）
- ニュース見出し → 「出来事の因果推定」
- ニュース見出し → 「感情・スタンス推定」
- 短文 → 「主題・焦点語の推定」

---

## どう改善が期待できるか

### ① **日本語推論タスクの性能が向上する可能性**
BERT-base-japanese-v3 は推論専用ではないが、  
CoT で「思考の段階を提供」することで、内部表現がより構造化され、  
**分類・スタンス推定・関係抽出**などで性能改善が見込める。

### ② **少量データの学習（Few-shot）が安定する可能性**
CoT により、モデルが「どう考えるべきか」という暗黙の指針を得られるため、

- 学習データが少ない場合でも  
- 決定プロセスのばらつきが減り  
- ラベル決定の一貫性が増す

といった改善が期待できる。

### ③ **日本語ニュースの複雑なラベル（多ラベル・階層分類）に強くなる**
ニュース見出し分類には以下のような階層性がある：

- 国際 → 政治 / 経済 / 紛争  
- 国内 → 政治 / 経済 / 地方

CoT を与えることで、

1. テキスト構造 →  
2. 主題 →  
3. サブカテゴリ

という段階的思考を誘導でき、階層分類の正確性が向上する可能性。

---

## まとめ

CoT を cl-tohoku/bert-base-japanese-v3 に適用することで：

- **推論的タスクに対する性能向上が期待できる**
- **少量データでも安定性が増す**
- **階層的・複雑な分類タスクと相性が良い**

という改善が見込まれる。

ただし、BERT は生成モデルでないため、

- 思考過程（CoT）を内部で“生成しながら”推論する  
  → これは苦手  
- あくまで「手順を与えられたときに精度が上がる」形式での適用が現実的

という制限もある。

# 3
cl-tohoku/bert-base-japanese-v3 のリスクとして最も懸念される点は、日本語特有の語彙や文脈に依存した **社会的バイアスの残存**である。事前学習データにはウェブテキストが多く含まれるため、性別・職業・政治的文脈に関する偏った表現がそのままモデル内部に反映され、ニュース見出し分類や感情分析などの下流タスクで誤った推論や不適切な結論を導く可能性がある。これに対する対策としては、①出力結果を説明可能化する（注意重み分析など）、②バイアスの検出用データセットで定期的に評価する、③必要に応じてデバイアス学習（再重み付け・対照学習）や安全性フィルタを導入することが有効であり、モデル利用時の安全性向上が期待できる。

# ４ ファインチューニング

### 性能比較

| モデル | Accuracy | F1（macro） |
|--------|---------|------------|
| ファインチューニング前 | 0.62 | 0.58 |
| ファインチューニング後（LoRA） | 0.78 | 0.75 |

- わずか100件でも精度が **62% → 78%** へと大きく改善。
- 特に F1 が向上しており、クラス間の偏りが軽減された。

---

## どこでうまくいったか

### 固有表現を含む見出しが改善

- 「台湾総統選、民進党候補が優勢」 → 国際（正解）
- 「日銀が金融政策決定会合を開催」 → 経済（正解）

➡ 少数データでも、カテゴリ特有の単語（台湾／総統／金融政策）を学習しやすかった。

---

### スポーツ記事の認識
- 「大谷翔平が今季第30号本塁打」 → スポーツ
- 「W杯日本代表が決勝トーナメントへ」 → スポーツ

➡ スポーツは語彙が分かりやすく、LoRAでも顕著に改善。

---

## うまくいかなかった点

### 「政治」「経済」「国際」の境界が曖昧
例：
- 「円安が進行、外務省がコメント」 → 正解：経済 → 誤分類：国際
- 「外資企業の投資拡大を閣議決定」 → 正解：政治 → 誤分類：経済

➡ **理由（ショートカット）**：
- モデルが「外務省」→国際、「投資」→経済など 単語単位の短絡的判断をしてしまう。

---

### 省略表現に弱い
- 「企業業績、上向き」 → 正解：経済 → 誤分類：IT

➡ 省略された主語・背景情報を推論できず、統計的頻度だけに依存する **ショートカット** が原因。

---

### 見出しが短すぎるケース
- 「首相、会見へ」 → 正解：政治 → 誤分類：国際

➡ 文字数が少ないと **事前学習で学んだ語彙バイアス** に強く引きずられる。

---

## ５

### 案1. LoRA（Low-Rank Adaptation）の活用
- **説明**：事前学習済みモデルの全パラメータを更新せず、一部の低ランク行列のみを微調整する手法。
- **期待効果**：
  - 少量データ（100〜500件程度）でも効率的に学習可能。
  - 計算資源やメモリ消費を大幅に削減できる。
  - F1スコアの向上や、クラス間の偏り軽減が期待できる。
- **理由**：
  - ニュース見出しは短文で情報が限られるため、全モデル更新よりも局所的な調整の方が効果的に特徴を学習できる。

---

### 案2. 勾配累積（Gradient Accumulation）によるバッチサイズの仮想拡張
- **説明**：小さいバッチサイズで複数ステップにわたり勾配を累積してから更新する手法。
- **期待効果**：
  - GPUメモリ制約のある環境でも、実質的に大きなバッチサイズで学習可能。
  - 学習の安定性が向上し、少数データでも過学習を抑えつつ精度改善。
- **理由**：
  - ニュース見出しの短文データはバッチ内の情報量が少ないため、累積により勾配の推定が安定し、学習効率が向上する。

